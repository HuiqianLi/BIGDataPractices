from pyspark.sql import Row
from pyspark.sql import SparkSession
import datetime

# ç­›é€‰10ï¼š00-11:00èµ·é£çš„èˆªç­
# å°†è¿è¡Œç»“æœå†™å…¥result.txtæ–‡ä»¶
file_handle=open('result.txt',mode='a+',encoding='utf-8')
file_handle.write('4.ç­›é€‰10ï¼š00-11:00èµ·é£çš„èˆªç­:\n')

session = SparkSession \
    .builder \
    .appName("4_TimeOver10") \
    .master('local[*]')\
    .config("spark.some.config.option", "some-value") \
    .getOrCreate()

sc = session.sparkContext
# å°†æ•°æ®æºè½¬æ¢ä¸ºRDD
originRDD = sc.textFile('20190315.csv')

rowRDD = originRDD.map(lambda x:str(x).split(','))\
    .map(lambda p:Row(\
    starting=p[0],\
    destination=p[1],\
    date=p[2],\
    company=p[4],\
    flight=p[5],\
    start_time=p[6],\
    arrive_time=p[7],\
    flying_time=str(p[6])[:2],\
    # T_Tä¸ä¼šè®¡ç®—æ—¶é—´å·® â†“
    # start_time=datetime.datetime.strptime(str(p[6]), "%H:%M"),\
    # arrive_time=datetime.datetime.strptime(str(p[7]), "%H:%M"),\
    # flying_time=(datetime.datetime.strptime(str(p[7]), "%H:%M")-datetime.datetime.strptime(str(p[6]), "%H:%M")).seconds,\
    punctuality_rate=p[8],\
    minprice=p[9]))
# ä»¥RDDä¸ºæ•°æ®æºï¼Œæ„å»ºDataFrame
df = session.createDataFrame(rowRDD)
# df.show()


df.createOrReplaceTempView('airplanes')

# 10ï¼š00-11:00èµ·é£çš„èˆªç­
df1 = session.sql('select * from airplanes where flying_time = "10"')

# ä»¥DataFrameä¸ºæ•°æ®æºï¼Œæ„å»ºRDDï¼Œå¹¶å°†åŸå§‹Rowç±»å‹æ•°æ®å†…å®¹è½¬æ¢ä¸ºåŸºæœ¬æ•°æ®åˆ—
transferRDD = df1.rdd.map(lambda row:(\
    'starting:' + row.starting, \
    'destination:' + row.destination,\
    'company:'+row.company,\
    'flight:'+row.flight,\
    'start_time:'+row.start_time))
# print(transferRDD.collect())

# # å¤ªå¤šäº†ğŸ˜­ï¼Œåªå–äºŒåä¸ª
# # for item in transferRDD.collect():
for item in transferRDD.take(20):
    file_handle.write(item.__str__() + '\n')

file_handle.write('-------------------------------------------------------------------\n')

file_handle.close()