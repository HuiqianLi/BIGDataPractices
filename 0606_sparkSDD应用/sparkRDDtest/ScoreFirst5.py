from pyspark import SparkContext,SparkConf

# æ„å»º é…ç½®å¯¹è±¡
conf = SparkConf().setAppName('ScoreFirst5').setMaster('local[*]')

# æ„å»º ç¯å¢ƒå¯¹è±¡
sc = SparkContext(conf=conf)

# è¯´æ˜ï¼šcsvæ–‡ä»¶ä¸ºscrapyçˆ¬è™«ä½œä¸šçˆ¬å–çš„æ¸¸æˆåˆ—è¡¨ğŸ™ƒ
# æ¯ä¸€åˆ—åˆ†åˆ«ä¸ºè‹±æ–‡åã€ä¸­æ–‡åã€å‘è¡Œæ—¥æœŸã€è¯„åˆ†ã€æ¸¸æˆåŸç½‘ç«™ã€ä»·æ ¼ã€steamè´­ä¹°/ä¸‹è½½é“¾æ¥
# English_name ã€Chinese_nameã€release_dateã€scoreã€websiteã€priceã€download_link

originRDD = sc.textFile('games.csv')

# å¼‚å¸¸æ•°æ®ï¼šç¬¬15è¡Œï¼š "Papers, Please",å…¥å¢ƒç®¡ç†å¤„,
# çˆ¬è™«çš„æ—¶å€™ç”¨\tåˆ†å§ï¼Œä¸çŸ¥é“æ€ä¹ˆåˆ¤æ–­å®ƒæ˜¯å¦å¼‚å¸¸â€¦â€¦ï¼Œæ”¹æˆ"Papers Please"äº†
    
def handle(line):
    x = line.split(",")
    return x[1], x[3]
    # return x[1], int(x[3])

# å°†è¾“å‡ºç»“æœå†™å…¥result.txtæ–‡ä»¶
file_handle=open('result.txt',mode='a+',encoding='utf-8')
file_handle.write('2.è®¡ç®—è¯„åˆ†æœ€é«˜å‰5åï¼š\n')

# è®¡ç®—è¯„åˆ†æœ€é«˜å‰5å
MapRDD = originRDD.map(lambda line: handle(line))

sortRDD = MapRDD.sortBy(lambda x: x[1], False)

dataRDD = sortRDD.take(5)

for item in dataRDD:
    file_handle.write(str(item) + '\n')

file_handle.write('-------------------------------------\n')

file_handle.close()