from pyspark import SparkContext,SparkConf

# æ„å»º é…ç½®å¯¹è±¡
conf = SparkConf().setAppName('AveragePrice').setMaster('local[*]')

# æ„å»º ç¯å¢ƒå¯¹è±¡
sc = SparkContext(conf=conf)

# è¯´æ˜ï¼šcsvæ–‡ä»¶ä¸ºscrapyçˆ¬è™«ä½œä¸šçˆ¬å–çš„æ¸¸æˆåˆ—è¡¨ğŸ™ƒ
# æ¯ä¸€åˆ—åˆ†åˆ«ä¸ºè‹±æ–‡åã€ä¸­æ–‡åã€å‘è¡Œæ—¥æœŸã€è¯„åˆ†ã€æ¸¸æˆåŸç½‘ç«™ã€ä»·æ ¼ã€steamè´­ä¹°/ä¸‹è½½é“¾æ¥
# English_name ã€Chinese_nameã€release_dateã€scoreã€websiteã€priceã€download_link

originRDD = sc.textFile('games.csv')

# å¼‚å¸¸æ•°æ®ï¼šç¬¬15è¡Œï¼š "Papers, Please",å…¥å¢ƒç®¡ç†å¤„,
# çˆ¬è™«çš„æ—¶å€™ç”¨\tåˆ†å§ï¼Œä¸çŸ¥é“æ€ä¹ˆåˆ¤æ–­å®ƒæ˜¯å¦å¼‚å¸¸â€¦â€¦ï¼Œæ”¹æˆ"Papers Please"äº†
    
def handle(line):
    x = line.split(",")
    return int(str(x[5])[1:])

# å°†è¾“å‡ºç»“æœå†™å…¥result.txtæ–‡ä»¶
file_handle=open('result.txt',mode='a+',encoding='utf-8')
file_handle.write('4.è®¡ç®—å¹³å‡ä»·æ ¼ï¼š\n')

# è®¡ç®—å¹³å‡ä»·æ ¼
MapRDD = originRDD.map(lambda line: handle(line))

sum = 0
for item in MapRDD.collect():
    sum += item
    # file_handle.write(str(item) + '\n')
file_handle.write('ï¿¥' + str(sum/MapRDD.count()) + '\n')

file_handle.write('-------------------------------------\n')

file_handle.close()