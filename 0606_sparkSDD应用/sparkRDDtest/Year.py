from pyspark import SparkContext,SparkConf

# æ„å»º é…ç½®å¯¹è±¡
conf = SparkConf().setAppName('Year').setMaster('local[*]')

# æ„å»º ç¯å¢ƒå¯¹è±¡
sc = SparkContext(conf=conf)

# è¯´æ˜ï¼šcsvæ–‡ä»¶ä¸ºscrapyçˆ¬è™«ä½œä¸šçˆ¬å–çš„æ¸¸æˆåˆ—è¡¨ğŸ™ƒ
# æ¯ä¸€åˆ—åˆ†åˆ«ä¸ºè‹±æ–‡åã€ä¸­æ–‡åã€å‘è¡Œæ—¥æœŸã€è¯„åˆ†ã€æ¸¸æˆåŸç½‘ç«™ã€ä»·æ ¼ã€steamè´­ä¹°/ä¸‹è½½é“¾æ¥
# English_name ã€Chinese_nameã€release_dateã€scoreã€websiteã€priceã€download_link

originRDD = sc.textFile('games.csv')

# å¼‚å¸¸æ•°æ®ï¼šç¬¬15è¡Œï¼š "Papers, Please",å…¥å¢ƒç®¡ç†å¤„,
# çˆ¬è™«çš„æ—¶å€™ç”¨\tåˆ†å§ï¼Œä¸çŸ¥é“æ€ä¹ˆåˆ¤æ–­å®ƒæ˜¯å¦å¼‚å¸¸â€¦â€¦ï¼Œæ”¹æˆ"Papers Please"äº†
    
def handle(line):
    x = line.split(",")
    return int(str(x[2])[:4]), x[1]

# å°†è¾“å‡ºç»“æœå†™å…¥result.txtæ–‡ä»¶
file_handle=open('result.txt',mode='a+',encoding='utf-8')
file_handle.write('5.å‘è¡Œå¹´ä»½ç»Ÿè®¡ï¼š\n')

# å‘è¡Œå¹´ä»½ç»Ÿè®¡
# MapRDD = originRDD.map(lambda line: handle(line))

# filterRDD = MapRDD.filter(lambda x: x[1])

# # sortRDD = MapRDD.sortBy(lambda x: x[1], False)

# for item in filterRDD.collect():
#     file_handle.write(str(item) + '\n')

MapRDD = originRDD.map(lambda line: handle(line))

reduceByKeyRDD = MapRDD.reduceByKey(lambda a, b: a + ',' + b)
# æŒ‰ç…§é”®æ’åº
sortByKeyRdd = reduceByKeyRDD.sortByKey()
for item in sortByKeyRdd.collect():
    file_handle.write(str(item) + '\n')


file_handle.write('-------------------------------------\n')

file_handle.close()