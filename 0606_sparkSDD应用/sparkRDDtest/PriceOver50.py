from pyspark import SparkContext,SparkConf

# æž„å»º é…ç½®å¯¹è±¡
conf = SparkConf().setAppName('PriceOver50').setMaster('local[*]')

# æž„å»º çŽ¯å¢ƒå¯¹è±¡
sc = SparkContext(conf=conf)

# è¯´æ˜Žï¼šcsvæ–‡ä»¶ä¸ºscrapyçˆ¬è™«ä½œä¸šçˆ¬å–çš„æ¸¸æˆåˆ—è¡¨ðŸ™ƒ
# æ¯ä¸€åˆ—åˆ†åˆ«ä¸ºè‹±æ–‡åã€ä¸­æ–‡åã€å‘è¡Œæ—¥æœŸã€è¯„åˆ†ã€æ¸¸æˆåŽŸç½‘ç«™ã€ä»·æ ¼ã€steamè´­ä¹°/ä¸‹è½½é“¾æŽ¥
# English_name ã€Chinese_nameã€release_dateã€scoreã€websiteã€priceã€download_link

originRDD = sc.textFile('games.csv')

# å¼‚å¸¸æ•°æ®ï¼šç¬¬15è¡Œï¼š "Papers, Please",å…¥å¢ƒç®¡ç†å¤„,
# çˆ¬è™«çš„æ—¶å€™ç”¨\tåˆ†å§ï¼Œä¸çŸ¥é“æ€Žä¹ˆåˆ¤æ–­å®ƒæ˜¯å¦å¼‚å¸¸â€¦â€¦ï¼Œæ”¹æˆ"Papers Please"äº†
    
def handle(line):
    x = line.split(",")
    return x[1], x[5]
    # return x[1], int(x[3])

# å°†è¾“å‡ºç»“æžœå†™å…¥result.txtæ–‡ä»¶
file_handle=open('result.txt',mode='a+',encoding='utf-8')
file_handle.write('3.è®¡ç®—ä»·æ ¼é«˜äºŽ50çš„ï¼š\n')

# è®¡ç®—ä»·æ ¼é«˜äºŽ50çš„
MapRDD = originRDD.map(lambda line: handle(line))

filterRDD = MapRDD.filter(lambda x: int(str(x[1])[1:]) >= 50)#.map(lambda x: x[0])

for item in filterRDD.collect():
    file_handle.write(str(item) + '\n')

file_handle.write('-------------------------------------\n')

file_handle.close()